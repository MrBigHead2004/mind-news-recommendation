{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6942e9eb",
   "metadata": {},
   "source": [
    "# MIND News Recommendation System\n",
    "\n",
    "## Architecture Overview:\n",
    "1. **News Encoder:** BERT/RoBERTa + Attention Pooling (Title → Vector)\n",
    "2. **User Encoder:** Attention Pooling over history (History → User Vector)\n",
    "3. **Prediction:** Dot product similarity + Softmax\n",
    "4. **Training:** Negative Sampling with Cross-Entropy Loss\n",
    "\n",
    "## Pipeline:\n",
    "```\n",
    "User History → News Encoder → User Encoder → User Vector\n",
    "                                                  ↓\n",
    "Candidate News → News Encoder → Candidate Vectors → Dot Product → Click Probability\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc67be",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e260463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet -r requirements.txt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from src import *\n",
    "\n",
    "# Set style for better plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085b8c94",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f687e071",
   "metadata": {},
   "source": [
    "## Download dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a78e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c0081",
   "metadata": {},
   "source": [
    "## News\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e7555",
   "metadata": {},
   "source": [
    "### Load and Explore News Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b09d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = load_news_data()\n",
    "\n",
    "print(f\"\\nNews Articles Samples:\")\n",
    "news_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d3e49",
   "metadata": {},
   "source": [
    "### Tokenize News Articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b108a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initializing tokenizer: {config['MODEL_NAME']}...\")\n",
    "news_features, news_categories = tokenize_news(news_df)\n",
    "print(f\"Tokenized {len(news_features):,} news articles (including <PAD>)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22989647",
   "metadata": {},
   "source": [
    "## Create Training Samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd56d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating training samples...\")\n",
    "print(f\"Strategy: 1 positive + {config['NEG_SAMPLES']} negative samples per training example\")\n",
    "\n",
    "train_behaviors_df, val_behaviors_df = load_behaviors_data()\n",
    "\n",
    "train_samples = create_behavior_samples(train_behaviors_df, 'train')\n",
    "val_samples = create_behavior_samples(val_behaviors_df, 'val')\n",
    "\n",
    "print(f\"\\nCreated {len(train_samples):,} training samples from {config['BEHAVIORS_TRAIN_PATH']}\")\n",
    "print(f\"Created {len(val_samples):,} validation samples from {config['BEHAVIORS_VAL_PATH']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c7d00",
   "metadata": {},
   "source": [
    "## Create PyTorch Dataset and DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8348b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for news recommendation with category support.\"\"\"\n",
    "    \n",
    "    def __init__(self, samples, news_features, news_categories, config):\n",
    "        self.samples = samples\n",
    "        self.news_features = news_features\n",
    "        self.news_categories = news_categories\n",
    "        self.config = config\n",
    "        self.use_category = config.get('USE_CATEGORY_ATTENTION', False)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Process history\n",
    "        history_ids = sample['history']\n",
    "        if len(history_ids) < self.config['MAX_HISTORY_LEN']:\n",
    "            history_ids = history_ids + ['<PAD>'] * (self.config['MAX_HISTORY_LEN'] - len(history_ids))\n",
    "        \n",
    "        hist_input_ids = torch.cat([\n",
    "            self.news_features.get(nid, self.news_features['<PAD>'])['input_ids'] \n",
    "            for nid in history_ids\n",
    "        ])\n",
    "        hist_attn_mask = torch.cat([\n",
    "            self.news_features.get(nid, self.news_features['<PAD>'])['attention_mask'] \n",
    "            for nid in history_ids\n",
    "        ])\n",
    "        \n",
    "        # Process candidates\n",
    "        candidate_ids = sample['candidates']\n",
    "        cand_input_ids = torch.cat([\n",
    "            self.news_features.get(nid, self.news_features['<PAD>'])['input_ids'] \n",
    "            for nid in candidate_ids\n",
    "        ])\n",
    "        cand_attn_mask = torch.cat([\n",
    "            self.news_features.get(nid, self.news_features['<PAD>'])['attention_mask'] \n",
    "            for nid in candidate_ids\n",
    "        ])\n",
    "        \n",
    "        result = {\n",
    "            'history_input_ids': hist_input_ids,\n",
    "            'history_attn_mask': hist_attn_mask,\n",
    "            'candidate_input_ids': cand_input_ids,\n",
    "            'candidate_attn_mask': cand_attn_mask,\n",
    "            'label': torch.tensor(sample['label'], dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        # Add categories if enabled\n",
    "        if self.use_category:\n",
    "            hist_categories = torch.tensor([\n",
    "                self.news_categories.get(nid, 0) for nid in history_ids\n",
    "            ], dtype=torch.long)\n",
    "            cand_categories = torch.tensor([\n",
    "                self.news_categories.get(nid, 0) for nid in candidate_ids\n",
    "            ], dtype=torch.long)\n",
    "            result['history_categories'] = hist_categories\n",
    "            result['candidate_categories'] = cand_categories\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating PyTorch datasets...\")\n",
    "train_dataset = NewsDataset(train_samples, news_features, news_categories, config)\n",
    "val_dataset = NewsDataset(val_samples, news_features, news_categories, config)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset):,} samples\")\n",
    "print(f\"Validation dataset: {len(val_dataset):,} samples\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config['BATCH_SIZE'], \n",
    "    shuffle=True,\n",
    "    num_workers=0  # Set to 0 to avoid multiprocessing issues\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,  # Variable number of candidates per sample\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87feb1e",
   "metadata": {},
   "source": [
    "## Initialize Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(config['MODEL_TYPE'], config).to(config['DEVICE'])\n",
    "\n",
    "for param in model.news_encoder.embedding.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# # Unfreeze only the last N transformer layers\n",
    "# num_layers_to_unfreeze = 2  # Adjust this (try 1-3)\n",
    "# for layer in model.news_encoder.embedding.encoder.layer[-num_layers_to_unfreeze:]:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['LR'])\n",
    "# optimizer = torch.optim.AdamW([\n",
    "#     # Pretrained RoBERTa - lower learning rate\n",
    "#     {'params': model.news_encoder.embedding.parameters(), 'lr': 1e-5},\n",
    "#     # Custom attention layers - higher learning rate\n",
    "#     {'params': model.news_encoder.attention_pooling.parameters(), 'lr': 1e-4},\n",
    "#     {'params': model.user_attention.parameters(), 'lr': 1e-4}\n",
    "# ], weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model initialized on {config['DEVICE']}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fdfc4d",
   "metadata": {},
   "source": [
    "## Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e47a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, train_loader, val_loader, optimizer, criterion, config['DEVICE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f57de",
   "metadata": {},
   "source": [
    "## Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fdb6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint to get training history\n",
    "checkpoint = torch.load('mind_news_rec.pth', map_location=config['DEVICE'])\n",
    "history = checkpoint['history']\n",
    "\n",
    "# Print final metrics\n",
    "print(\"\\nFinal Training Results:\")\n",
    "print(f\"Final Training Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Best Validation AUC: {max(history['val_auc']):.4f}\")\n",
    "print(f\"Best Validation MRR: {max(history['val_mrr']):.4f}\")\n",
    "print(f\"Best Validation NDCG@5: {max(history['val_ndcg@5']):.4f}\")\n",
    "print(f\"Best Validation NDCG@10: {max(history['val_ndcg@10']):.4f}\")\n",
    "\n",
    "# Print loss values for each epoch\n",
    "print(\"\\nLoss per Epoch:\")\n",
    "print(\"=\" * 30)\n",
    "for i, (epoch, loss) in enumerate(zip(history['epoch'], history['train_loss'])):\n",
    "    print(f\"Epoch {epoch:2d}: {loss:.6f}\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Print model configuration for reference\n",
    "print(\"\\nModel Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in checkpoint['config'].items():\n",
    "    print(f\"{key:25s} : {value}\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
