{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6942e9eb",
   "metadata": {},
   "source": [
    "# MIND News Recommendation System\n",
    "\n",
    "## Architecture Overview:\n",
    "1. **News Encoder:** BERT/RoBERTa + Attention Pooling (Title → Vector)\n",
    "2. **User Encoder:** Attention Pooling over history (History → User Vector)\n",
    "3. **Prediction:** Dot product similarity + Softmax\n",
    "4. **Training:** Negative Sampling with Cross-Entropy Loss\n",
    "\n",
    "## Pipeline:\n",
    "```\n",
    "User History → News Encoder → User Encoder → User Vector\n",
    "                                                  ↓\n",
    "Candidate News → News Encoder → Candidate Vectors → Dot Product → Click Probability\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc67be",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e260463e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lenhathoang/miniconda3/envs/recsys/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet numpy==1.26.4 pandas==2.3.3 matplotlib seaborn tqdm torch transformers scikit-learn sentence-transformers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from src import *\n",
    "\n",
    "# Set style for better plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085b8c94",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede6b39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CONFIGURATION\n",
      "==================================================\n",
      "NEWS_TRAIN_PATH      : /Users/lenhathoang/Developer/HUST Projects/mind-news-recommendation/MIND/MINDsmall_train/news.tsv\n",
      "NEWS_VAL_PATH        : /Users/lenhathoang/Developer/HUST Projects/mind-news-recommendation/MIND/MINDsmall_dev/news.tsv\n",
      "BEHAVIORS_TRAIN_PATH : /Users/lenhathoang/Developer/HUST Projects/mind-news-recommendation/MIND/MINDsmall_train/behaviors.tsv\n",
      "BEHAVIORS_VAL_PATH   : /Users/lenhathoang/Developer/HUST Projects/mind-news-recommendation/MIND/MINDsmall_dev/behaviors.tsv\n",
      "CHECKPOINT_PATH      : /Users/lenhathoang/Developer/HUST Projects/mind-news-recommendation/checkpoints/mind_news_rec.pth\n",
      "MODEL_NAME           : roberta-base\n",
      "EMBEDDING_DIM        : 768\n",
      "ATTENTION_QUERY_DIM  : 200\n",
      "MAX_TITLE_LEN        : 10\n",
      "MAX_HISTORY_LEN      : 20\n",
      "NEG_SAMPLES          : 4\n",
      "BATCH_SIZE           : 8\n",
      "EPOCHS               : 3\n",
      "LR                   : 0.0001\n",
      "DEVICE               : mps\n",
      "LOAD_CHECKPOINT      : False\n",
      "DEBUG_SUBSET_SIZE    : 100\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f687e071",
   "metadata": {},
   "source": [
    "## Download dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "961a78e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/nhthongl/mind-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 54.0M/1.25G [00:07<02:53, 7.42MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdownload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/HUST Projects/mind-news-recommendation/src/download_dataset.py:7\u001b[0m, in \u001b[0;36mdownload_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdownload_dataset\u001b[39m():\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    Download dataset from Kaggle.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43mkagglehub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_download\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnhthongl/mind-dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath to dataset files:\u001b[39m\u001b[38;5;124m\"\u001b[39m, path)\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.9/site-packages/kagglehub/datasets.py:43\u001b[0m, in \u001b[0;36mdataset_download\u001b[0;34m(handle, path, force_download)\u001b[0m\n\u001b[1;32m     41\u001b[0m h \u001b[38;5;241m=\u001b[39m parse_dataset_handle(handle)\n\u001b[1;32m     42\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;241m.\u001b[39mto_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mEXTRA_CONSOLE_BLOCK})\n\u001b[0;32m---> 43\u001b[0m path, _ \u001b[38;5;241m=\u001b[39m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.9/site-packages/kagglehub/registry.py:28\u001b[0m, in \u001b[0;36mMultiImplRegistry.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impls):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mis_supported(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m         fails\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(impl)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.9/site-packages/kagglehub/resolver.py:29\u001b[0m, in \u001b[0;36mResolver.__call__\u001b[0;34m(self, handle, path, force_download)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m, handle: T, path: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, force_download: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resolves a handle into a path with the requested file(s) and the resource's version number.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m        Some cases where version number might be missing: Competition datasource, API-based models.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     path, version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     register_datasource_access(handle, version)\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.9/site-packages/kagglehub/http_resolver.py:130\u001b[0m, in \u001b[0;36mDatasetHttpResolver._resolve\u001b[0;34m(self, h, path, force_download)\u001b[0m\n\u001b[1;32m    127\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(archive_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# First, we download the archive.\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchive_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m _extract_archive(archive_path, out_path)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Delete the archive\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.9/site-packages/kagglehub/clients.py:217\u001b[0m, in \u001b[0;36mKaggleApiV1Client.download_file\u001b[0;34m(self, path, out_file, resource_handle, cached_path, extract_auto_compressed_file)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 217\u001b[0m     \u001b[43m_download_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hash_object:\n\u001b[1;32m    220\u001b[0m     actual_md5_hash \u001b[38;5;241m=\u001b[39m to_b64_digest(hash_object)\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.9/site-packages/kagglehub/clients.py:276\u001b[0m, in \u001b[0;36m_download_file\u001b[0;34m(response, out_file, size_read, total_size, hash_object)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mtotal_size, initial\u001b[38;5;241m=\u001b[39msize_read, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, unit_divisor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m progress_bar:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_file, open_mode) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 276\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(CHUNK_SIZE):\n\u001b[1;32m    277\u001b[0m             f\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m hash_object:\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.9/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.9/site-packages/urllib3/response.py:1091\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1091\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1093\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1094\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.9/site-packages/urllib3/response.py:980\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 980\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.9/site-packages/urllib3/response.py:904\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    901\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    906\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.9/site-packages/urllib3/response.py:887\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 887\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.9/http/client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.9/socket.py:716\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 716\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.9/ssl.py:1277\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1274\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1275\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/recsys/lib/python3.9/ssl.py:1135\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c0081",
   "metadata": {},
   "source": [
    "## News\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e7555",
   "metadata": {},
   "source": [
    "### Load and Explore News Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b09d165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading News Articles from /Users/lenhathoang/Developer/HUST Projects/mind-news-recommendation/MIND/MINDsmall_train/news.tsv and /Users/lenhathoang/Developer/HUST Projects/mind-news-recommendation/MIND/MINDsmall_dev/news.tsv...\n",
      "  Training news: 51,282 articles\n",
      "  Validation news: 42,416 articles\n",
      "Loaded 65,238 unique news articles (combined)\n",
      "\n",
      "News Articles Samples:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N55528</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N19639</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N61837</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N53526</td>\n",
       "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N38324</td>\n",
       "      <td>How to Get Rid of Skin Tags, According to a De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>N2073</td>\n",
       "      <td>Should NFL be able to fine players for critici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>N49186</td>\n",
       "      <td>It's been Orlando's hottest October ever so fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>N59295</td>\n",
       "      <td>Chile: Three die in supermarket fire amid prot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>N24510</td>\n",
       "      <td>Best PS5 games: top PlayStation 5 titles to lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>N39237</td>\n",
       "      <td>How to report weather-related closings, delays</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  news_id                                              title\n",
       "0  N55528  The Brands Queen Elizabeth, Prince Charles, an...\n",
       "1  N19639                      50 Worst Habits For Belly Fat\n",
       "2  N61837  The Cost of Trump's Aid Freeze in the Trenches...\n",
       "3  N53526  I Was An NBA Wife. Here's How It Affected My M...\n",
       "4  N38324  How to Get Rid of Skin Tags, According to a De...\n",
       "5   N2073  Should NFL be able to fine players for critici...\n",
       "6  N49186  It's been Orlando's hottest October ever so fa...\n",
       "7  N59295  Chile: Three die in supermarket fire amid prot...\n",
       "8  N24510  Best PS5 games: top PlayStation 5 titles to lo...\n",
       "9  N39237     How to report weather-related closings, delays"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = load_news_data()\n",
    "\n",
    "print(f\"\\nNews Articles Samples:\")\n",
    "news_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d3e49",
   "metadata": {},
   "source": [
    "### Tokenize News Articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6b108a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tokenizer: roberta-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 65238/65238 [00:06<00:00, 10439.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 65,239 news articles (including <PAD>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initializing tokenizer: {config['MODEL_NAME']}...\")\n",
    "news_features = tokenize_news(news_df)\n",
    "print(f\"Tokenized {len(news_features):,} news articles (including <PAD>)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22989647",
   "metadata": {},
   "source": [
    "## Create Training Samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acd56d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training samples...\n",
      "Strategy: 1 positive + 4 negative samples per training example\n",
      "Training dataset path: /Users/lenhathoang/Developer/HUST Projects/mind-news-recommendation/MIND/MINDsmall_train/behaviors.tsv\n",
      "Validation dataset path: /Users/lenhathoang/Developer/HUST Projects/mind-news-recommendation/MIND/MINDsmall_dev/behaviors.tsv\n",
      "DEBUG MODE: Using 100 train and 100 val behaviors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating train samples: 100%|██████████| 100/100 [00:00<00:00, 29435.78it/s]\n",
      "Creating val samples: 100%|██████████| 100/100 [00:00<00:00, 59773.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 137 training samples from /Users/lenhathoang/Developer/HUST Projects/mind-news-recommendation/MIND/MINDsmall_train/behaviors.tsv\n",
      "Created 100 validation samples from /Users/lenhathoang/Developer/HUST Projects/mind-news-recommendation/MIND/MINDsmall_dev/behaviors.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating training samples...\")\n",
    "print(f\"Strategy: 1 positive + {config['NEG_SAMPLES']} negative samples per training example\")\n",
    "\n",
    "train_behaviors_df, val_behaviors_df = load_behaviors_data()\n",
    "\n",
    "train_samples = create_behavior_samples(train_behaviors_df, 'train')\n",
    "val_samples = create_behavior_samples(val_behaviors_df, 'val')\n",
    "\n",
    "print(f\"\\nCreated {len(train_samples):,} training samples from {config['BEHAVIORS_TRAIN_PATH']}\")\n",
    "print(f\"Created {len(val_samples):,} validation samples from {config['BEHAVIORS_VAL_PATH']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c7d00",
   "metadata": {},
   "source": [
    "## Create PyTorch Dataset and DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8348b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PyTorch datasets...\n",
      "Train dataset: 137 samples\n",
      "Validation dataset: 100 samples\n",
      "Train batches: 18\n",
      "Validation batches: 100\n"
     ]
    }
   ],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for news recommendation\"\"\"\n",
    "    def __init__(self, samples, news_features, config):\n",
    "        self.samples = samples\n",
    "        self.news_features = news_features\n",
    "        self.config = config\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Process history\n",
    "        history_ids = sample['history']\n",
    "        # Pad if shorter than max length\n",
    "        if len(history_ids) < self.config['MAX_HISTORY_LEN']:\n",
    "            history_ids = history_ids + ['<PAD>'] * (self.config['MAX_HISTORY_LEN'] - len(history_ids))\n",
    "        \n",
    "        # Stack history features\n",
    "        hist_input_ids = torch.cat([\n",
    "            self.news_features.get(nid, self.news_features['<PAD>'])['input_ids'] \n",
    "            for nid in history_ids\n",
    "        ])\n",
    "        hist_attn_mask = torch.cat([\n",
    "            self.news_features.get(nid, self.news_features['<PAD>'])['attention_mask'] \n",
    "            for nid in history_ids\n",
    "        ])\n",
    "        \n",
    "        # Process candidates\n",
    "        candidate_ids = sample['candidates']\n",
    "        cand_input_ids = torch.cat([\n",
    "            self.news_features.get(nid, self.news_features['<PAD>'])['input_ids'] \n",
    "            for nid in candidate_ids\n",
    "        ])\n",
    "        cand_attn_mask = torch.cat([\n",
    "            self.news_features.get(nid, self.news_features['<PAD>'])['attention_mask'] \n",
    "            for nid in candidate_ids\n",
    "        ])\n",
    "        \n",
    "        return {\n",
    "            'history_input_ids': hist_input_ids,\n",
    "            'history_attn_mask': hist_attn_mask,\n",
    "            'candidate_input_ids': cand_input_ids,\n",
    "            'candidate_attn_mask': cand_attn_mask,\n",
    "            'label': torch.tensor(sample['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating PyTorch datasets...\")\n",
    "train_dataset = NewsDataset(train_samples, news_features, config)\n",
    "val_dataset = NewsDataset(val_samples, news_features, config)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset):,} samples\")\n",
    "print(f\"Validation dataset: {len(val_dataset):,} samples\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config['BATCH_SIZE'], \n",
    "    shuffle=True,\n",
    "    num_workers=0  # Set to 0 to avoid multiprocessing issues\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,  # Variable number of candidates per sample\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87feb1e",
   "metadata": {},
   "source": [
    "## Initialize Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f57f119c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized on mps\n",
      "Total parameters: 124,953,632\n",
      "Trainable parameters: 124,953,632\n"
     ]
    }
   ],
   "source": [
    "model = NewsRecommender(config).to(config['DEVICE'])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['LR'])\n",
    "# optimizer = torch.optim.AdamW([\n",
    "#     # Pretrained RoBERTa - lower learning rate\n",
    "#     {'params': model.news_encoder.embedding.parameters(), 'lr': 1e-5},\n",
    "#     # Custom attention layers - higher learning rate\n",
    "#     {'params': model.news_encoder.attention_pooling.parameters(), 'lr': 1e-4},\n",
    "#     {'params': model.user_attention.parameters(), 'lr': 1e-4}\n",
    "# ], weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model initialized on {config['DEVICE']}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fdfc4d",
   "metadata": {},
   "source": [
    "## Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1e47a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING FOR 3 EPOCHS\n",
      "==================================================\n",
      "\n",
      "\n",
      "Epoch 1/3\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 18/18 [00:07<00:00,  2.54it/s, loss=2.6741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 7.7125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:03<00:00, 25.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:\n",
      "  AUC: 0.5131\n",
      "  MRR@5: 0.2458\n",
      "  MRR@10: 0.2842\n",
      "  NDCG@5: 0.2610\n",
      "  NDCG@10: 0.3531\n",
      "Model saved to /Users/lenhathoang/Developer/HUST Projects/mind-news-recommendation/checkpoints/mind_news_rec.pth\n",
      "\n",
      "\n",
      "Epoch 2/3\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 18/18 [00:06<00:00,  2.79it/s, loss=1.8432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.6280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:03<00:00, 30.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:\n",
      "  AUC: 0.5488\n",
      "  MRR@5: 0.2517\n",
      "  MRR@10: 0.2789\n",
      "  NDCG@5: 0.2640\n",
      "  NDCG@10: 0.3385\n",
      "Model saved to /Users/lenhathoang/Developer/HUST Projects/mind-news-recommendation/checkpoints/mind_news_rec.pth\n",
      "\n",
      "\n",
      "Epoch 3/3\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 18/18 [00:06<00:00,  2.84it/s, loss=1.0546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.6075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:03<00:00, 30.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:\n",
      "  AUC: 0.5312\n",
      "  MRR@5: 0.2172\n",
      "  MRR@10: 0.2336\n",
      "  NDCG@5: 0.2525\n",
      "  NDCG@10: 0.3040\n",
      "Model saved to /Users/lenhathoang/Developer/HUST Projects/mind-news-recommendation/checkpoints/mind_news_rec.pth\n",
      "\n",
      "==================================================\n",
      "TRAINING COMPLETE!\n",
      "==================================================\n",
      "Final Training Loss: 1.6075\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, optimizer, criterion, config['DEVICE'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
